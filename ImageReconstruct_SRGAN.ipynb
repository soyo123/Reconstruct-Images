{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Interface(数据处理)"
      ],
      "metadata": {
        "id": "mZ6Z0DPw636n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ZaSUgFU-M5"
      },
      "outputs": [],
      "source": [
        "from os import listdir # For file operations\n",
        "from os.path import join # For catalog operations\n",
        "from PIL import Image # For image processing\n",
        "from torch.utils.data.dataset import Dataset # Base class for defining datasets\n",
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize # For image preprocessing\n",
        "\n",
        "#--- Used to determine if a file name is an image file ---#\n",
        "def is_image_file(filename):\n",
        "    return any(\n",
        "        filename.endswith(extension)\n",
        "        for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']\n",
        "    ) # Checks if the filename suffix is a common image file format, if so, returns True, otherwise returns False\n",
        "\n",
        "#--- Adjust the cut size according to the given cut size and upsampling factor ---#\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor) # An integer multiple of the crop size is obtained by subtracting it from the remainder of the upsampling factor\n",
        "\n",
        "#--- Training set high-resolution graph preprocessing function ---#\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size), # Perform random cropping operations\n",
        "        ToTensor(), # Converting images to tensors\n",
        "    ])\n",
        "\n",
        "#--- Training set low-resolution map preprocessing function ---#\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        ToPILImage(), # Convert tensor to PIL image\n",
        "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC), # Resize the image and interpolate using bicubic interpolation (BICUBIC)\n",
        "        ToTensor() # Converting images to tensors\n",
        "    ])\n",
        "\n",
        "#--- Preprocessing operations for displaying images ---#\n",
        "def display_transform():\n",
        "    return Compose(\n",
        "        [ToPILImage(), # Converting images to PIL images\n",
        "         Resize(400), # Resize the image to 400x400\n",
        "         CenterCrop(400), # Perform Center Trimming\n",
        "         ToTensor()] # Converting images to tensors\n",
        "    )\n",
        "\n",
        "#--- Training dataset class ---#\n",
        "class TrainDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, crop_size, upscale_factor): # Initialize the dataset based on the given dataset_dir, crop_size and upscale_factor\n",
        "        super(TrainDatasetFromFolder, self).__init__()\n",
        "        self.image_filenames = [\n",
        "            join(dataset_dir, x) for x in listdir(dataset_dir)\n",
        "            if is_image_file(x)\n",
        "        ]  # Filter the image files by traversing the files in dataset_dir and save them in the image_filenames list\n",
        "        crop_size = calculate_valid_crop_size(crop_size,upscale_factor) # Calculate a suitable crop size, making sure that the crop size is divisible by the upscale_factor.\n",
        "        self.hr_transform = train_hr_transform(crop_size)  # High-resolution map preprocessor function\n",
        "        self.lr_transform = train_lr_transform(crop_size,upscale_factor)  # Low resolution map preprocessing function\n",
        "\n",
        "    # Return low-resolution images and high-resolution images as training samples\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))  # Randomized cropping and pre-processing to obtain high resolution maps\n",
        "        lr_image = self.lr_transform(hr_image)  # Preprocessing to obtain low-resolution maps\n",
        "        return lr_image, hr_image # Returns low-resolution images and high-resolution images\n",
        "\n",
        "    # Returns the size of the dataset, i.e. the number of image files\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "#--- Validation dataset ---#\n",
        "class ValDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor):\n",
        "        super(ValDatasetFromFolder, self).__init__()\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.image_filenames = [\n",
        "            join(dataset_dir, x) for x in listdir(dataset_dir)\n",
        "            if is_image_file(x)\n",
        "        ] # Here the function code is explained as above\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = Image.open(self.image_filenames[index]) # Index which opens high-resolution images\n",
        "        w, h = hr_image.size # Get the width w and height h of the image\n",
        "        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor) # Calculate a suitable crop size, making sure that the crop size is divisible by the upscale_factor.\n",
        "        lr_scale = Resize(crop_size // self.upscale_factor,interpolation=Image.BICUBIC) # Adjusting the image size\n",
        "        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
        "        hr_image = CenterCrop(crop_size)(hr_image)  # Center cropping of high-resolution images\n",
        "        lr_image = lr_scale(hr_image)  # Obtaining low-resolution maps\n",
        "        hr_restore_img = hr_scale(lr_image) # Recovered high-resolution image\n",
        "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image) # The processed image data are all converted to a tensor, and the low-resolution image, the recovered high-resolution image, and the original high-resolution image are returned as validation samples\n",
        "\n",
        "    # Returns the size of the dataset, i.e. the number of image files\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "#--- Test Data Set ---#\n",
        "class TestDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor):\n",
        "        super(TestDatasetFromFolder, self).__init__()\n",
        "        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/' # Constructed a list of file paths for low-resolution images\n",
        "        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/' # Constructed a list of file paths for high-resolution images\n",
        "        self.upscale_factor = upscale_factor # Constructed directory paths to hold low-resolution images and high-resolution images\n",
        "        self.lr_filenames = [\n",
        "            join(self.lr_path, x) for x in listdir(self.lr_path)\n",
        "            if is_image_file(x)\n",
        "        ]\n",
        "        self.hr_filenames = [\n",
        "            join(self.hr_path, x) for x in listdir(self.hr_path)\n",
        "            if is_image_file(x)\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_name = self.lr_filenames[index].split('/')[-1]\n",
        "        lr_image = Image.open(self.lr_filenames[index]) # Get the image name from the low-resolution image file list and open the image\n",
        "        w, h = lr_image.size\n",
        "        hr_image = Image.open(self.hr_filenames[index]) # Get the image name from the list of high-resolution image files and open the image\n",
        "        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w),interpolation=Image.BICUBIC) # Enlargement recovery operation for low-resolution images\n",
        "        hr_restore_img = hr_scale(lr_image) # Obtaining recovered high-resolution images\n",
        "        return image_name, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lr_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator（生成器）"
      ],
      "metadata": {
        "id": "MlDUTj9MFBzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math # For math calculations\n",
        "import torch\n",
        "from torch import nn # Importing the nn (neural network) module\n",
        "\n",
        "#--- Generator Model ---#\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor): # Accepts a parameter scale_factor, representing the upsampling factor\n",
        "        upsample_block_num = int(math.log(scale_factor, 2)) # The number of upsampling blocks is calculated based on the upsampling factor. Here the upsampling block is the module used in the generator to recover the resolution\n",
        "\n",
        "        super(Generator, self).__init__() # Initialize the base class of the generator\n",
        "        self.block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=9, padding=4),nn.PReLU()) # The first convolutional layer with a convolutional kernel size of 9×9, the number of input channels is 3 and the number of output channels is 64. nn.PReLU activation function\n",
        "        # 6 residual blocks\n",
        "        self.block2 = ResidualBlock(64) # Setting the number of input channels to 64 indicates that the output from the first convolutional block accesses the\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64)) # Sequence module containing convolution and batch normalization layers nn.BatchNorm2d.\n",
        "        # upsample_block_num upsample blocks\n",
        "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)] # Recover 2x upsampling multiplier per upsampling module\n",
        "        # The last convolutional layer with a convolutional kernel size of 9×9, 64 input channels and 3 output channels\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x): # Define the forward propagation method of the generator with input x, denoting the input image\n",
        "        block1 = self.block1(x) # Pass the input image through the first convolution block block1\n",
        "\n",
        "        # Pass the input image layer by layer to get the feature map in the middle block7\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        # The output of the first convolution block, block1, is summed with the output of the seventh block, block7, and then passed through the upsampling module sequence, block8.\n",
        "        block8 = self.block8(block1 + block7)\n",
        "\n",
        "        return (torch.tanh(block8) + 1) / 2 # The final generated feature map block8 is range-mapped by the Tanh activation function to values between [-1,1] and then mapped to [0,1] by some operations\n",
        "\n",
        "#--- Residual blocks in the generator ---#\n",
        "# In the super-resolution task, residual blocks are used to learn the details and textures of the image\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # Two convolutional layers with a convolutional kernel size of 3×3 and a constant number of channels to keep the feature map size constant\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels) # Batch normalization layer for accelerating the training process and stabilizing the learning of the network\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x): # Define the forward propagation method of the generator with input x, denoting the input image\n",
        "        # The first passes through the first convolutional layer, batch normalization and activation function, followed by the second convolutional layer and batch normalization. Finally, the inputs and residuals are summed, resulting in a residual join\n",
        "        residual = self.conv1(x)\n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.prelu(residual)\n",
        "        residual = self.conv2(residual)\n",
        "        residual = self.bn2(residual)\n",
        "\n",
        "        return x + residual\n",
        "\n",
        "#--- Upsampling blocks in the generator---#\n",
        "# In the super-resolution task, the upsampling block is used to increase the resolution of the image\n",
        "class UpsampleBLock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBLock, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, # Number of input channels\n",
        "            in_channels * up_scale**2, # Number of output channels, each with an enhanced resolution of 2\n",
        "            kernel_size=3, # Convolution kernel size\n",
        "            padding=1\n",
        "        ) # Convolutional layer for upsampling the low-resolution feature maps\n",
        "        self.pixel_shuffle = nn.PixelShuffle(up_scale) # Pixel rearrangement layer for performing upsampling operations. It converts the input low-resolution feature map into a high-resolution image\n",
        "        self.prelu = nn.PReLU() # Correcting the linear cell activation function\n",
        "\n",
        "    def forward(self, x): # Define the forward propagation method of the generator\n",
        "        # The feature map is upsampled by a convolutional layer, then the final upsampling operation is performed using a pixel rearrangement layer, and finally the activation function is applied\n",
        "        x = self.conv(x)\n",
        "        x = self.pixel_shuffle(x)\n",
        "        x = self.prelu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "f2MC8lDwWWLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator（判别器）"
      ],
      "metadata": {
        "id": "ht7VEI9PKS8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- Discriminator Model ---#\n",
        "# Used to discriminate the generated images to determine if they are similar to real images\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # A series of nn.Conv2d: defines a series of convolutional layers that are used to extract features from the image. Each convolutional layer alternates between nn.BatchNorm2d for batch normalization and nn.LeakyReLU as an activation function.\n",
        "\n",
        "            # 1st convolutional layer with convolutional kernel size 3×3, number of input channels 3, number of output channels 64\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 2nd convolutional layer with 3×3 convolutional kernel size, 64 input channels and 64 output channels\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 3rd convolutional layer with a convolutional kernel size of 3×3, 64 input channels and 128 output channels\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 4th convolutional layer with 3×3 convolutional kernel size, 128 input channels and 128 output channels\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 5th convolutional layer with a convolutional kernel size of 3×3, 128 input channels and 256 output channels\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 6th convolutional layer with a convolutional kernel size of 3×3, 256 input channels and 256 output channels\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 7th convolutional layer with a convolutional kernel size of 3×3, 256 input channels and 512 output channels\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # The 8th convolutional layer with a convolutional kernel size of 3×3 and a number of input channels of 512 and output channels of 512\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # Global pooling layer for converting feature maps to global feature vectors\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            # Two fully-connected layers, implemented using convolution, are used to map a global feature vector to a scalar value, which is used to discriminate whether the input image is real or not\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1))\n",
        "\n",
        "    def forward(self, x): # Define the forward propagation method of the generator\n",
        "        # Through a series of convolutional and pooling layers, the global features are finally mapped to a scalar output through a convolutional layer\n",
        "        batch_size = x.size(0)\n",
        "        return torch.sigmoid(self.net(x).view(batch_size)) # Restricting the output to the range (0, 1) indicates the probability of the discriminatory result"
      ],
      "metadata": {
        "id": "hKjuAhnEWjOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the loss function（定义损失函数）"
      ],
      "metadata": {
        "id": "dJhdBnyHte_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn # Neural network module\n",
        "from torchvision.models.vgg import vgg16 # Importing the VGG-16 model\n",
        "import os # For operating the operating system\n",
        "os.environ['TORCH_HOME'] = './' # The TORCH_HOME environment variable will be set to the current directory, which will be used to specify the path where PyTorch downloads pre-trained models\n",
        "\n",
        "\n",
        "#--- Definition of generator loss function ---#\n",
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True) # Loading the pre-trained VGG-16 model\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval() # Extracting the 31 layers of the former VGG-16 model as a loss network\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()  # Mean square error loss function MSE\n",
        "        self.tv_loss = TVLoss()  # TV smoothing loss function\n",
        "\n",
        "    # Adversarial loss, perceptual loss, image MSE loss and TV smoothing loss were calculated and summed with certain weights to form the combined loss of the generator\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        adversarial_loss = torch.mean(1 - out_labels) # Against loss, the generator outputs labels out_labels with the difference between the mean of the target labels and 1\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images),self.loss_network(target_images)) # Perceptual loss, the generated image and the target image are fed into the loss network (the first 31 layers of vgg16) separately and their mean square error losses are calculated\n",
        "        image_loss = self.mse_loss(out_images, target_images) # Image MSE loss, the difference between the generated image and the target image target_images\n",
        "        tv_loss = self.tv_loss(out_images) # TV smoothing loss\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss # Return the weighted sum of the four losses as the total loss of the generator\n",
        "\n",
        "#--- Definition of TV smoothing loss function ---#\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight # Denotes the weight of the TV smoothing loss, default is 1\n",
        "\n",
        "    def forward(self, x): # Define the forward propagation method of the generator\n",
        "        # Get the batch size, height and width of the input tensor\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        # Indicates the number of pixel pairs in the vertical and horizontal directions used to normalize the loss value\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        # Calculate the TV smoothing loss in the vertical and horizontal directions\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        # Return weighted TV smoothing loss\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3] # Calculate the number of elements of the input tensor\n",
        "\n",
        "# Create an instance of the generator loss function and print the output\n",
        "if __name__ == \"__main__\":\n",
        "    g_loss = GeneratorLoss()\n",
        "    print(g_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yszSta4W4bm",
        "outputId": "0d0cb965-909b-43d9-9b07-94039c0a65a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to ./hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:14<00:00, 38.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeneratorLoss(\n",
            "  (loss_network): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (mse_loss): MSELoss()\n",
            "  (tv_loss): TVLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the optimizer（定义优化器）"
      ],
      "metadata": {
        "id": "vrDNVDpByUFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp # Introducing the exponential function exp for Gaussian functions\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F # Import PyTorch's functional interface module for calling functions such as convolution operations\n",
        "from torch.autograd import Variable # Import the Variable class from the AutoDerivative module to create the variable that stores the gradient\n",
        "\n",
        "#--- Define the Gaussian function---#\n",
        "def gaussian(window_size, sigma): # Create a Gaussian weight tensor based on window size and standard deviation\n",
        "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2))\n",
        "                          for x in range(window_size)]) # Calculate the value of the Gaussian function by looping through it and store the result in the gauss tensor\n",
        "    return gauss / gauss.sum() # Normalize the gauss tensor to ensure that the weights sum to 1\n",
        "\n",
        "#--- Creating Gaussian Window Functions ---#\n",
        "def create_window(window_size, channel): # Create a 2D Gaussian window tensor based on window size and number of channels\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1) # Create a one-dimensional Gaussian weight tensor _1D_window and add a dimension to the first dimension\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0) # Multiplying the one-dimensional weight tensor with its transpose gives the two-dimensional Gaussian window tensor\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous()) # Extending the 2D Gaussian window tensor to a shape that matches the number of input image channels\n",
        "    return window\n",
        "\n",
        "#--- Calculation of the Structural Similarity Index (SSIM)---#\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average=True): # Accepts two images, img1 and img2, and the previously created Gaussian window window to calculate the SSIM value\n",
        "    # Calculate the mean of the input image and window using the convolution operation (F.conv2d)\n",
        "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
        "\n",
        "    # Calculate the square of the mean\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    # Calculate the variance (sigma1_sq and sigma2_sq) and covariance (sigma12) of the image using the convolution operation\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01 ** 2\n",
        "    C2 = 0.03 ** 2\n",
        "\n",
        "    # Calculating SSIM images\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    # Returns a value representing the structural similarity between images\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "#--- Defining SSIM Classes---#\n",
        "# For the calculation of the Structural Similarity Index (SSIM)\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True): # Initialization method to create an instance of the SSIM class\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size # Used to store the size of the Gaussian window\n",
        "        self.size_average = size_average # Used to store whether results are averaged\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel) # Creates a Gaussian window, calling the previously defined create_window function\n",
        "\n",
        "    def forward(self, img1, img2): # Forward propagation method to compute the structural similarity index (SSIM) between the input images img1 and img2\n",
        "        (_, channel, _, _) = img1.size() # Get the dimension information of the input image img1, channel stores the number of channels.\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type(): # Check that the current number of channels is the same as the number of channels previously stored, and that the Gaussian window previously created is on the same device\n",
        "            window = self.window # Use the previously created Gaussian window directly.\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel) # Create a new Gaussian window based on the current number of channels and Gaussian window size\n",
        "\n",
        "            if img1.is_cuda: # Check if the input image img1 is on the GPU\n",
        "                window = window.cuda(img1.get_device()) # Move the Gaussian window to the same GPU device as the input image\n",
        "            window = window.type_as(img1) # Set the data type of the Gaussian window to the same as the input image\n",
        "\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average) # Calculate the structural similarity between images and return the result\n",
        "\n",
        "def ssim(img1, img2, window_size=11, size_average=True): # Used to calculate the structural similarity index (SSIM) between the input images img1 and img2\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel) # Create a Gaussian window\n",
        "\n",
        "    if img1.is_cuda: # Check if the input image img1 is on the GPU\n",
        "        window = window.cuda(img1.get_device()) # Move the Gaussian window to the same GPU device as the input image\n",
        "    window = window.type_as(img1) # Set the data type of the Gaussian window to the same as the input image\n",
        "\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average) # Calculate the structural similarity between images and return the result"
      ],
      "metadata": {
        "id": "YsfpzDYgW-WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training（模型训练）"
      ],
      "metadata": {
        "id": "9OqcJ4b0Oi_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # For processing file paths\n",
        "from math import log10 # Used to calculate logarithmic values\n",
        "\n",
        "import pandas as pd # For processing and analyzing data\n",
        "import torch.optim as optim # Used to define the optimizer\n",
        "import torch.utils.data # Used to define and manipulate data loaders\n",
        "import torchvision.utils as utils # For processing image data\n",
        "from torch.autograd import Variable # Automatic Derivative Functions for Defining Variables\n",
        "from torch.utils.data import DataLoader # Used to create data loaders\n",
        "from tqdm import tqdm # Used to display a progress bar in a loop\n",
        "\n",
        "\n",
        "if __name__ == '__main__': # Check if the current script is executed in the main program\n",
        "    CROP_SIZE = 240 #opt.crop_size # crop_size, the size of the image crop used for training\n",
        "    UPSCALE_FACTOR = 4 # oversampling multiplier\n",
        "    NUM_EPOCHS = 4 # Iteration epoch number\n",
        "\n",
        "    # Get training set/validation set image data\n",
        "    train_set = TrainDatasetFromFolder('/content/drive/MyDrive/ImageNet', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
        "    val_set = ValDatasetFromFolder('/content/drive/MyDrive/Colab Notebooks/data/ImageNet', upscale_factor=UPSCALE_FACTOR)\n",
        "    # DataLoader for training and validation sets created through DataLoader\n",
        "    train_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=16, shuffle=True) # Data loader for training, each batch contains 16 images that are randomly shuffled\n",
        "    val_loader = DataLoader(dataset=val_set, num_workers=0, batch_size=1, shuffle=False) # Data loader for validation, one image at a time, no shuffling\n",
        "\n",
        "    netG = Generator(UPSCALE_FACTOR) # Generator Definitions\n",
        "    netD = Discriminator() # Discriminator Definition\n",
        "    generator_criterion = GeneratorLoss() # Generator Loss Function\n",
        "\n",
        "    # Whether to use the GPU and move the model and loss function to the GPU\n",
        "    if torch.cuda.is_available():\n",
        "        netG.cuda()\n",
        "        netD.cuda()\n",
        "        generator_criterion.cuda()\n",
        "\n",
        "    # Generator and discriminator optimizers for optimizing generator and discriminator parameters\n",
        "    optimizerG = optim.Adam(netG.parameters())\n",
        "    optimizerD = optim.Adam(netD.parameters())\n",
        "\n",
        "    # For storing loss and evaluation metrics during training, including generator loss, discriminator loss, discriminator score, generator score, PSNR (Peak Signal-to-Noise Ratio), and SSIM (Structural Similarity Index)\n",
        "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "    # A for loop is used to iterate through each epoch, performing training and loss calculations on each batch.\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        train_bar = tqdm(train_loader) # Use tqdm to create a progress bar, train_bar, that displays training progress\n",
        "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0} # Used to keep track of some statistical information in the current epoch, including batch_sizes (number of images currently processed), d_loss (discriminator loss), g_loss (generator loss), d_score (discriminator score), g_score (generator score)\n",
        "\n",
        "        netG.train() # Generator training\n",
        "        netD.train() # Discriminator training\n",
        "\n",
        "        # Each epoch of data iteration, traversing each batch in the training dataset\n",
        "        for data, target in train_bar:\n",
        "            g_update_first = True # Flag to control whether the generator is updated first\n",
        "            batch_size = data.size(0) # Get the size of the current batch\n",
        "            running_results['batch_sizes'] += batch_size # Add the number of images of the current batch to the batch_sizes field of valing_results\n",
        "\n",
        "            # Optimize the discriminator to maximize D(x)-1-D(G(z))\n",
        "            real_img = Variable(target) # The goal of preserving real images\n",
        "            if torch.cuda.is_available():\n",
        "                real_img = real_img.cuda() # Some pre-processing of the real image, e.g. moving the data to the GPU (if available)\n",
        "            z = Variable(data) # Save the target of the generated image\n",
        "            if torch.cuda.is_available():\n",
        "                z = z.cuda()\n",
        "            fake_img = netG(z) # Generate fake_img (super-resolution image) from input data z (low-resolution image) via generator netG\n",
        "            netD.zero_grad() # Zeroing the parameter gradient of the discriminator\n",
        "            # Use netD to discriminate between the real image and the generated image to get real_out and fake_out respectively (output of the discriminator)\n",
        "            real_out = netD(real_img).mean()\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            d_loss = 1 - real_out + fake_out # Calculate the discriminator loss d_loss, which is used to maximize D(x)-1- D(G(z)), i.e., to encourage the discriminator to correctly discriminate between the real image and the generated image\n",
        "            # Backpropagation and optimization of discriminator parameters to reduce d_loss\n",
        "            d_loss.backward(retain_graph=True)\n",
        "            optimizerD.step() # optimization discriminator\n",
        "\n",
        "            # Optimization Generator Minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
        "            netG.zero_grad() # Zeroing the parameter gradient of the generator\n",
        "            fake_img = netG(z)\n",
        "            fake_out = netD(fake_img).mean()\n",
        "            g_loss = generator_criterion(fake_out, fake_img, real_img) # Calculate generator loss g_loss, including perceptual loss, image loss and smoothing loss\n",
        "\n",
        "            # Backpropagation and optimization of generator parameters to reduce g_loss\n",
        "            g_loss.backward()\n",
        "\n",
        "            optimizerG.step() # Optimization Generator\n",
        "\n",
        "            # Calculate the various losses and scores for the current batch\n",
        "            running_results['g_loss'] += g_loss.item() * batch_size\n",
        "            running_results['d_loss'] += d_loss.item() * batch_size\n",
        "            running_results['d_score'] += real_out.item() * batch_size\n",
        "            running_results['g_score'] += fake_out.item() * batch_size\n",
        "            train_bar.set_description(\n",
        "                desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "                running_results['g_loss'] / running_results['batch_sizes'],\n",
        "                running_results['d_score'] / running_results['batch_sizes'],\n",
        "                running_results['g_score'] / running_results['batch_sizes']\n",
        "                )\n",
        "            ) # Update the information displayed in the progress bar, including the current epoch, the total number of epochs, the discriminator loss, the generator loss, the real image score and the generated image score, etc.\n",
        "\n",
        "        # Validating the validation set\n",
        "        netG.eval() # Switch generator netG to validation mode\n",
        "        out_path = '/content/drive/MyDrive/training_results/SRF_' + str(UPSCALE_FACTOR) + '/' # Create a directory out_path to store the validation results; UPSCALE_FACTOR is the super-resolution upsampling multiplier used to construct the storage paths\n",
        "        if not os.path.exists(out_path):\n",
        "            os.makedirs(out_path) # If the directory path out_path does not exist, create the directory out_path.\n",
        "\n",
        "        # Calculate validation set related metrics\n",
        "        with torch.no_grad(): # means that no gradient computation will be performed in the following statement block to save memory and computational resources\n",
        "            val_bar = tqdm(val_loader) # Use tqdm to create a progress bar, val_bar, that displays the progress of the evaluation on the validation set\n",
        "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0} # Used to save metrics from the validation process, including mse (mean square error), ssims (sum of structural similarity metrics), psnr (peak signal-to-noise ratio), and ssim (structural similarity metrics)\n",
        "            val_images = [] # Create an empty list to store the super-resolution images on the validation set\n",
        "            for val_lr, val_hr_restore, val_hr in val_bar: # Start a loop that traverses each sample in the validation set\n",
        "                batch_size = val_lr.size(0) # Get the size of the current batch\n",
        "                valing_results['batch_sizes'] += batch_size # Add the number of images of the current batch to the batch_sizes field of valing_results\n",
        "                lr = val_lr # Low resolution truth map\n",
        "                hr = val_hr # High Resolution Truth Map\n",
        "                if torch.cuda.is_available():\n",
        "                    lr = lr.cuda() # Moving low-resolution truth maps to the GPU\n",
        "                    hr = hr.cuda() # Moving high-resolution truth maps to the GPU\n",
        "                sr = netG(lr) # Super-resolution reconstruction of low-resolution images\n",
        "\n",
        "                batch_mse = ((sr - hr) ** 2).data.mean() # Calculates the mean square error (MSE) of the current batch, which measures the difference between the generated super-resolution image and the true high-resolution image\n",
        "                valing_results['mse'] += batch_mse * batch_size # Add the MSE of the current batch multiplied by the batch size to the mse field of valing_results\n",
        "                valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes'])) # Calculates the peak signal-to-noise ratio (PSNR) metric for the current batch, which measures the quality of the image reconstruction\n",
        "                batch_ssim = ssim(sr, hr).item() # 计Calculates the structural similarity index (SSIM) of the current batch, which measures the structural similarity between the generated super-resolution image and the real high-resolution image\n",
        "                valing_results['ssims'] += batch_ssim * batch_size # Add the SSIM of the current batch multiplied by the batch size to the ssims field of valing_results\n",
        "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes'] # Calculate and update the average structural similarity metric (SSIM) in valing_results\n",
        "        # Store generator, discriminator model parameters\n",
        "        torch.save(netG.state_dict(), '/content/drive/MyDrive/epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch)) #  UPSCALE_FACTOR为超分辨率倍率，epoch为当前 epoch数\n",
        "        torch.save(netD.state_dict(), '/content/drive/MyDrive/epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
        "        # Record the training set loss as well as the psnr,ssim and other metrics of the validation set \\scores\\psnr\\ssim\n",
        "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes']) # Add the average of the discriminator loss divided by the batch size for the current epoch to the d_loss list in the results dictionary results\n",
        "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
        "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes']) # Add the average of the discriminator scores from the current epoch divided by the batch size to the d_score list in the results dictionary results\n",
        "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
        "        results['psnr'].append(valing_results['psnr']) # Add the PSNR metrics from the validation set in the current epoch to the psnr list in the results dictionary results\n",
        "        results['ssim'].append(valing_results['ssim'])\n",
        "\n",
        "        # Store results to a local file\n",
        "        if epoch % 10 == 0 and epoch != 0:\n",
        "            out_path = '/content/drive/MyDrive/statistics' # Setting the path to the directory where statistics are stored\n",
        "            data_frame = pd.DataFrame( # Save the loss and evaluation metrics from the training process into a DataFrame\n",
        "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
        "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']}, # Create a dictionary containing the names of the loss and evaluation indicators as keys and the corresponding lists as values\n",
        "                index=range(1, epoch + 1)) # Set the index of the DataFrame to 1 to the current epoch number.\n",
        "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch') # Save the data in the DataFrame as a CSV file, with UPSCALE_FACTOR as the super resolution factor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZE0Gi4uX3uB",
        "outputId": "2dd9d5a0-a9b2-4d2c-b1b5-7c01c350494d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1/4] Loss_D: 0.9761 Loss_G: 0.0181 D(x): 0.6265 D(G(z)): 0.5953: 100%|██████████| 196/196 [06:52<00:00,  2.11s/it]\n",
            "100%|██████████| 345/345 [00:23<00:00, 14.92it/s]\n",
            "[2/4] Loss_D: 1.0008 Loss_G: 0.0097 D(x): 0.7308 D(G(z)): 0.7296: 100%|██████████| 196/196 [04:43<00:00,  1.45s/it]\n",
            "100%|██████████| 345/345 [00:23<00:00, 14.88it/s]\n",
            "[3/4] Loss_D: 0.9994 Loss_G: 0.0076 D(x): 0.9190 D(G(z)): 0.9207: 100%|██████████| 196/196 [04:44<00:00,  1.45s/it]\n",
            "100%|██████████| 345/345 [00:23<00:00, 14.79it/s]\n",
            "[4/4] Loss_D: 1.0000 Loss_G: 0.0069 D(x): 0.9997 D(G(z)): 0.9997: 100%|██████████| 196/196 [04:44<00:00,  1.45s/it]\n",
            "100%|██████████| 345/345 [00:23<00:00, 14.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Testing(模型测试)"
      ],
      "metadata": {
        "id": "DyqCsPHlUe9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # For building and manipulating deep learning models\n",
        "from PIL import Image # For image reading and processing\n",
        "from torch.autograd import Variable # Variables for wrapping images into computable gradients\n",
        "from torchvision.transforms import ToTensor, ToPILImage # For converting between PyTorch Tensor and PIL images\n",
        "\n",
        "UPSCALE_FACTOR = 4 # upsampling multiplier\n",
        "TEST_MODE = True # Testing with GPUs\n",
        "\n",
        "IMAGE_NAME = \"/content/drive/MyDrive/SampleImage/NO.20class4deer.jpg\"  # Test Image Path\n",
        "MODEL_NAME = '/content/drive/MyDrive/epochs/netG_epoch_4_4.pth' # The path of a trained model\n",
        "model = Generator(UPSCALE_FACTOR).eval() # Create a generator model and set it to validation mode\n",
        "\n",
        "if TEST_MODE:\n",
        "    model.cuda() # Moving the model to the GPU\n",
        "    model.load_state_dict(torch.load(MODEL_NAME),False) # Load the weights of the trained generator model\n",
        "else:\n",
        "    model.load_state_dict(torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)) # Load model weights and move the model to the CPU\n",
        "\n",
        "image = Image.open(IMAGE_NAME) # Read the image to be tested\n",
        "# image = Variable(ToTensor()(image), volatile=True).unsqueeze(0) # Image Preprocessing\n",
        "image = ToTensor()(image).unsqueeze(0)\n",
        "if TEST_MODE:\n",
        "    image = image.cuda() # Moving images to the GPU\n",
        "\n",
        "with torch.no_grad(): # Avoid calculating gradients\n",
        "    RESULT_NAME = \"/content/drive/MyDrive/ImageReconstructed\" + str(UPSCALE_FACTOR) + \"_\" + IMAGE_NAME.split(\"/\")[-1] # Constructs the filename of the output image\n",
        "    out = model(image) # The image is fed into the generator model for super-resolution reconstruction to get the output image\n",
        "    #out_img = ToPILImage()(out[0].data.cpu()) # Converting the output PyTorch Tensor to a PIL image\n",
        "    out_img = ToPILImage()(out[0].cpu())\n",
        "    out_img.save(RESULT_NAME) # Save the output image to a file"
      ],
      "metadata": {
        "id": "d5yQm5wSGo4l"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}